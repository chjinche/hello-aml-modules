{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Training Pipeline\n",
    "\n",
    "In this sample, image preprocessing is on cpu nodes while training on distributed gpu nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.pipeline.wrapper import Module, Pipeline, dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure workspace and compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configure workspace information here.\n",
    "workspace = Workspace.get(\n",
    "    name='itp-pilot',\n",
    "    subscription_id='4aaa645c-5ae2-4ae9-a17a-84b9023bc56a',\n",
    "    resource_group='itp-pilot-ResGrp'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify aml compute name.\n",
    "gpu_compute_target = 'gpu-nd24s'\n",
    "# gpu_compute_target = 'k80-16-a'\n",
    "cpu_compute_target = 'compute-d14'\n",
    "try:\n",
    "    gpu_compute = AmlCompute(workspace, gpu_compute_target)\n",
    "    print(\"Found existing gpu compute target: {}\".format(gpu_compute_target))\n",
    "except:\n",
    "    print(\"Need to create a new gpu compute\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_NC6\",\n",
    "                                                                min_nodes = 0, \n",
    "                                                                max_nodes = 4)\n",
    "    gpu_compute = ComputeTarget.create(workspace, gpu_compute_target, provisioning_config)\n",
    "    gpu_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "try:\n",
    "    cpu_compute = AmlCompute(workspace, cpu_compute_target)\n",
    "    print(\"Found existing cpu compute target: {}\".format(cpu_compute_target))\n",
    "except:\n",
    "    print(\"Creating a new cpu compute target: {}\".format(cpu_compute_target))  \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_DS3_V2\",\n",
    "                                                                min_nodes = 0, \n",
    "                                                                max_nodes = 4)    \n",
    "    cpu_compute = ComputeTarget.create(workspace, cpu_compute_target, provisioning_config)\n",
    "    cpu_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "This smaller imagenet dataset is a subset of the official one.\n",
    "- training dataset contains 1.2m images (1000 categories * 1200 images per category)\n",
    "- validation dataset contains 50k images (1000 categories * 50 images per category)\n",
    "\n",
    "Need to use zip file here to avoid perf issue of mounting file dataset with many subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "train_image_dataset = Dataset.get_by_name(workspace, name='ImageNetFullTrainData')\n",
    "val_image_dataset = Dataset.get_by_name(workspace, name='ImageNetFullValidData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load built-in modules\n",
    "convert_func = Module.load(workspace, namespace='azureml', name='Convert to Image Directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_transform_func = Module.load(workspace, namespace='azureml', name='Init Image Transformation')\n",
    "apply_transform_func = Module.load(workspace, namespace='azureml', name='Apply Image Transformation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local modules\n",
    "# this train module is a mpi module.\n",
    "module_folder = r'./modules'\n",
    "yaml_file_name = 'entry.spec.yaml'\n",
    "train_module = Module.from_yaml(workspace, yaml_file=f'{module_folder}/ConvNets/{yaml_file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "@dsl.pipeline(name='image classification', description='image classification', default_compute_target='compute-d14')\n",
    "def generated_pipeline():\n",
    "    convert_train = convert_func(\n",
    "        input_dataset=train_image_dataset\n",
    "    )\n",
    "    \n",
    "    convert_val = convert_func(\n",
    "        input_dataset=val_image_dataset\n",
    "    )\n",
    "    \n",
    "    init_trans = init_transform_func(\n",
    "        resize='False',\n",
    "        size=256,\n",
    "        center_crop='False',\n",
    "        crop_size=224,\n",
    "        pad='False',\n",
    "        padding=0,\n",
    "        color_jitter='False',\n",
    "        grayscale='False',\n",
    "        random_resized_crop='False',\n",
    "        random_resized_crop_size=256,\n",
    "        random_crop='False',\n",
    "        random_crop_size=224,\n",
    "        random_horizontal_flip='True',\n",
    "        random_vertical_flip='False',\n",
    "        random_rotation='False',\n",
    "        random_rotation_degrees=0,\n",
    "        random_affine='False',\n",
    "        random_affine_degrees=0,\n",
    "        random_grayscale='False',\n",
    "        random_perspective='False'\n",
    "    )\n",
    "    \n",
    "    apply_trans_on_train = apply_transform_func(\n",
    "        mode='For training',\n",
    "        input_image_transformation=init_trans.outputs.output_image_transformation,\n",
    "        input_image_directory=convert_train.outputs.output_image_directory\n",
    "    )\n",
    "    \n",
    "    apply_trans_on_val = apply_transform_func(\n",
    "        mode='For inference',\n",
    "        input_image_transformation=init_trans.outputs.output_image_transformation,\n",
    "        input_image_directory=convert_val.outputs.output_image_directory\n",
    "    )\n",
    "    \n",
    "    train = train_module(\n",
    "        train_data=apply_trans_on_train.outputs.output_image_directory,\n",
    "        valid_data=apply_trans_on_val.outputs.output_image_directory,\n",
    "        data_backend='pytorch',\n",
    "        pretrained_weights=None,\n",
    "        epochs=50,\n",
    "        seed=123,\n",
    "        batch_size=256,\n",
    "        print_freq=100,\n",
    "        # lr=4.096,\n",
    "        # optimizer_batch_size=4096,\n",
    "        save_checkpoint_epochs=10\n",
    "    )\n",
    "    # perform distributed training with 4 nodes.\n",
    "    # note: process_count_per_node should be 1 because this module will launch distributed processes based on node device count.\n",
    "    train.runsettings.configure(target=gpu_compute_target, node_count=4, process_count_per_node=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "pipeline = generated_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# validate pipeline and visualize the graph\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.submit(experiment_name='image_classification_full_set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('py36': conda)",
   "language": "python",
   "name": "python36964bitpy36conda29846a912b234ea68a88a663a5affd92"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}